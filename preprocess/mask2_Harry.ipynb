{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "executionInfo": {
     "elapsed": 8580,
     "status": "ok",
     "timestamp": 1717284141542,
     "user": {
      "displayName": "Yuhua Situ",
      "userId": "08664026452793101142"
     },
     "user_tz": 300
    },
    "id": "KUA2iplpyAro"
   },
   "source": [
    "first time in a new instance: need to install the following\n",
    "!pip install -U boto3\n",
    "!pip install -U s3fs \n",
    "!conda install geopandas --yes\n",
    "!conda install -c conda-forge netCDF4 gdal --yes\n",
    "!conda install -c conda-forge gdal netcdf4 libgdal --yes\n",
    "\n",
    "# running the two pip installs separately seems to work better somehow... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 1999,
     "status": "ok",
     "timestamp": 1717284132963,
     "user": {
      "displayName": "Yuhua Situ",
      "userId": "08664026452793101142"
     },
     "user_tz": 300
    },
    "id": "7vg9yCsgxlg2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from time import time\n",
    "from glob import glob\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import geopandas as gpd\n",
    "from osgeo import osr, gdal\n",
    "gdal.UseExceptions()\n",
    "\n",
    "import netCDF4 as nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import NoCredentialsError\n",
    "\n",
    "# set up s3 location\n",
    "s3 = boto3.client('s3', region_name='us-east-1')  # Replace 'us-west-2' with your AWS region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# some tests. keeping them for demo. skip when running jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-07 03:55:12 2918072384 2000-01_part1.nc\n",
      "2025-04-07 04:06:59 1571273216 2000-01_part2.nc\n",
      "2025-04-07 04:08:03 2729810144 2000-02_part1.nc\n",
      "2025-04-07 04:09:14 1469901156 2000-02_part2.nc\n",
      "2025-04-07 04:10:00 2918072384 2000-03_part1.nc\n",
      "2025-04-07 04:11:06 1571273220 2000-03_part2.nc\n",
      "2025-04-07 04:17:09 2823941260 2000-04_part1.nc\n",
      "2025-04-07 04:17:09 1520587184 2000-04_part2.nc\n",
      "2025-04-07 23:41:56 2918072384 2000-05_part1.nc\n",
      "2025-04-07 23:41:56 1571273220 2000-05_part2.nc\n",
      "2025-04-07 23:41:56 2823941264 2000-06_part1.nc\n",
      "2025-04-07 23:41:56 1520587188 2000-06_part2.nc\n",
      "2025-04-07 04:17:57 2918072384 2000-07_part1.nc\n",
      "2025-04-07 04:19:10 1571273220 2000-07_part2.nc\n",
      "2025-04-07 04:19:49 2918072384 2000-08_part1.nc\n",
      "2025-04-07 04:20:59 1571273220 2000-08_part2.nc\n",
      "2025-04-07 04:21:37 2823941264 2000-09_part1.nc\n",
      "2025-04-07 04:22:39 1520587188 2000-09_part2.nc\n",
      "2025-04-07 04:23:39 2918072380 2000-10_part1.nc\n",
      "2025-04-07 04:25:21 1571273216 2000-10_part2.nc\n",
      "2025-04-07 04:26:11 2823941264 2000-11_part1.nc\n",
      "2025-04-07 04:27:26 1520587188 2000-11_part2.nc\n",
      "2025-04-07 04:28:07 2918072384 2000-12_part1.nc\n",
      "2025-04-07 04:29:22 1571273220 2000-12_part2.nc\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://pepper-dataset/Temp/2000/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/fsspec/registry.py:275: UserWarning: Your installed version of s3fs is very old and known to cause\n",
      "severe performance issues, see also https://github.com/dask/dask/issues/10276\n",
      "\n",
      "To fix, you should specify a lower version bound on s3fs, or\n",
      "update the current installation.\n",
      "\n",
      "  warnings.warn(s3_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(425, 21)\n"
     ]
    }
   ],
   "source": [
    "test_uri = 's3://pepper-dataset/crop_data/crop_IOWA/Iowa_corn_2019.csv'\n",
    "df = pd.read_csv(test_uri)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws s3 ls s3://pepper-dataset/county_mask_data/mask_2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<osgeo.gdal.Dataset; proxy of <Swig Object of type 'GDALDatasetShadow *' at 0x7f7720f14600> >"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdal.Open('/vsis3/pepper-dataset/county_mask_data/mask_2/Illinois_Carroll.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/vsis3/pepper-dataset/Temp/2000/2000-01_part1.nc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mnc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/vsis3/pepper-dataset/Temp/2000/2000-01_part1.nc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m ds:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(ds\u001b[38;5;241m.\u001b[39mvariables\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:2521\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4.Dataset.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:2158\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4._ensure_nc_success\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/vsis3/pepper-dataset/Temp/2000/2000-01_part1.nc'"
     ]
    }
   ],
   "source": [
    "with nc.Dataset('/vsis3/pepper-dataset/Temp/2000/2000-01_part1.nc', 'r') as ds:\n",
    "    print(ds.variables.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "`/vsis3/pepper-dataset/Temp/2000/2000-01_part1.nc' not recognized as being in a supported file format. It could have been recognized by driver GRIB, but plugin gdal_GRIB.so is not available in your installation. You may install it with 'conda install -c conda-forge libgdal-grib'. The GDAL_DRIVER_PATH configuration option is not set.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m gdal_ds \u001b[38;5;241m=\u001b[39m \u001b[43mgdal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOpen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/vsis3/pepper-dataset/Temp/2000/2000-01_part1.nc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gdal_ds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFailed to open file\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/osgeo/gdal.py:9192\u001b[0m, in \u001b[0;36mOpen\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   9188\u001b[0m _WarnIfUserHasNotSpecifiedIfUsingExceptions()\n\u001b[1;32m   9189\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gdal\n\u001b[0;32m-> 9192\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_gdal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOpen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: `/vsis3/pepper-dataset/Temp/2000/2000-01_part1.nc' not recognized as being in a supported file format. It could have been recognized by driver GRIB, but plugin gdal_GRIB.so is not available in your installation. You may install it with 'conda install -c conda-forge libgdal-grib'. The GDAL_DRIVER_PATH configuration option is not set."
     ]
    }
   ],
   "source": [
    "gdal_ds = gdal.Open('/vsis3/pepper-dataset/Temp/2000/2000-01_part1.nc')\n",
    "if gdal_ds is None:\n",
    "    print('Failed to open file')\n",
    "else:\n",
    "    print(gdal_ds.GetMetadata())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['VRT', 'DERIVED', 'GTI', 'SNAP_TIFF', 'GTiff', 'COG', 'NITF', 'RPFTOC', 'ECRGTOC', 'HFA', 'SAR_CEOS', 'CEOS', 'JAXAPALSAR', 'GFF', 'ELAS', 'ESRIC', 'AIG', 'AAIGrid', 'GRASSASCIIGrid', 'ISG', 'SDTS', 'DTED', 'PNG', 'JPEG', 'MEM', 'JDEM', 'GIF', 'BIGGIF', 'ESAT', 'FITS', 'BSB', 'XPM', 'BMP', 'DIMAP', 'AirSAR', 'RS2', 'SAFE', 'PCIDSK', 'PCRaster', 'ILWIS', 'SGI', 'SRTMHGT', 'Leveller', 'Terragen', 'netCDF', 'HDF4', 'HDF4Image', 'ISIS3', 'ISIS2', 'PDS', 'PDS4', 'VICAR', 'TIL', 'ERS', 'JP2OpenJPEG', 'L1B', 'FIT', 'GRIB', 'RMF', 'WCS', 'WMS', 'MSGN', 'RST', 'GSAG', 'GSBG', 'GS7BG', 'COSAR', 'TSX', 'COASP', 'R', 'MAP', 'KMLSUPEROVERLAY', 'WEBP', 'PDF', 'Rasterlite', 'MBTiles', 'PLMOSAIC', 'CALS', 'WMTS', 'SENTINEL2', 'MRF', 'TileDB', 'PNM', 'DOQ1', 'DOQ2', 'PAux', 'MFF', 'MFF2', 'GSC', 'FAST', 'BT', 'LAN', 'CPG', 'NDF', 'EIR', 'DIPEx', 'LCP', 'GTX', 'LOSLAS', 'NTv2', 'CTable2', 'ACE2', 'SNODAS', 'KRO', 'ROI_PAC', 'RRASTER', 'BYN', 'NOAA_B', 'RIK', 'USGSDEM', 'GXF', 'KEA', 'BAG', 'S102', 'S104', 'S111', 'HDF5', 'HDF5Image', 'NWT_GRD', 'NWT_GRC', 'ADRG', 'SRP', 'BLX', 'PostGISRaster', 'SAGA', 'XYZ', 'HF2', 'OZI', 'CTG', 'ZMap', 'NGSGEOID', 'IRIS', 'PRF', 'EEDAI', 'EEDA', 'DAAS', 'SIGDEM', 'TGA', 'OGCAPI', 'STACTA', 'STACIT', 'NSIDCbin', 'GNMFile', 'GNMDatabase', 'ESRI Shapefile', 'MapInfo File', 'UK .NTF', 'LVBAG', 'OGR_SDTS', 'S57', 'DGN', 'OGR_VRT', 'Memory', 'CSV', 'NAS', 'GML', 'GPX', 'LIBKML', 'KML', 'GeoJSON', 'GeoJSONSeq', 'ESRIJSON', 'TopoJSON', 'Interlis 1', 'Interlis 2', 'OGR_GMT', 'GPKG', 'SQLite', 'WAsP', 'PostgreSQL', 'OpenFileGDB', 'DXF', 'CAD', 'FlatGeobuf', 'Geoconcept', 'GeoRSS', 'VFK', 'PGDUMP', 'OSM', 'GPSBabel', 'OGR_PDS', 'WFS', 'OAPIF', 'EDIGEO', 'SVG', 'Idrisi', 'XLS', 'ODS', 'XLSX', 'Elasticsearch', 'Carto', 'AmigoCloud', 'SXF', 'Selafin', 'JML', 'PLSCENES', 'CSW', 'VDV', 'GMLAS', 'MVT', 'NGW', 'MapML', 'GTFS', 'PMTiles', 'JSONFG', 'MiraMonVector', 'TIGER', 'AVCBin', 'AVCE00', 'GenBin', 'ENVI', 'EHdr', 'ISCE', 'Zarr', 'HTTP', 'NUMPY']\n"
     ]
    }
   ],
   "source": [
    "gdal.AllRegister()\n",
    "driver_list = [gdal.GetDriver(i).ShortName for i in range(gdal.GetDriverCount())]\n",
    "print(driver_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "syntax error, unexpected WORD_WORD, expecting SCAN_ATTR or SCAN_DATASET or SCAN_ERROR\n",
      "context: <?xml^ version=\"1.0\" encoding=\"UTF-8\"?><Error><Code>AccessDenied</Code><Message>Access Denied</Message><RequestId>W9YWK6ZF1EAHE547</RequestId><HostId>c5QxyL3bxFX9beXR6iNr9catK++EGTnaHtAkI40THvcazckA6jRpJGCqHm9S+Qf/w+dECquOpPI=</HostId></Error>\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno -78] NetCDF: Authorization failure: 's3://pepper-dataset/era5-unzipped/2020/2020-06_part1.nc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mnc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms3://pepper-dataset/era5-unzipped/2020/2020-06_part1.nc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m ds:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(ds\u001b[38;5;241m.\u001b[39mvariables\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:2469\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4.Dataset.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:2028\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4._ensure_nc_success\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno -78] NetCDF: Authorization failure: 's3://pepper-dataset/era5-unzipped/2020/2020-06_part1.nc'"
     ]
    }
   ],
   "source": [
    "with nc.Dataset('s3://pepper-dataset/era5-unzipped/2020/2020-06_part1.nc', 'r') as ds:\n",
    "    print(ds.variables.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gds = gdal.Open('/vsis3/pepper-dataset/era5-unzipped/2020/2020-06_part1.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NETCDF:\"/vsis3/pepper-dataset/era5-unzipped/2020/2020-06_part1.nc\":t2m', '[720x251x601] t2m (16-bit integer)'), ('NETCDF:\"/vsis3/pepper-dataset/era5-unzipped/2020/2020-06_part1.nc\":rsn', '[720x251x601] rsn (16-bit integer)')]\n",
      "['t2m', 'rsn', 'sde', 'stl1', 'stl2', 'stl3', 'stl4', 'tsn', 'swvl1', 'swvl2', 'swvl3', 'swvl4']\n"
     ]
    }
   ],
   "source": [
    "era_subsets = gds.GetSubDatasets()\n",
    "print(era_subsets[:2])\n",
    "varnames = [x[0].split(':')[-1] for x in era_subsets]\n",
    "print(varnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gds1 = gdal.Open(era_subsets[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "testarr = gds1.ReadAsArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NETCDF:\"/vsis3/pepper-dataset/era5-unzipped/2020/2020-06_part1.nc\":t2m\n",
      "t2m\n"
     ]
    }
   ],
   "source": [
    "desc = gds1.GetDescription()\n",
    "print(desc)\n",
    "print(desc.split(':')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gds1_meta = gds1.GetMetadata()\n",
    "gds1_meta.keys()\n",
    "time_set = eval(gds1_meta['NETCDF_DIM_time_VALUES'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(time_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "time_list = []\n",
    "for t1 in time_set:\n",
    "    t2 = dt.datetime(1900,1,1) + dt.timedelta(hours = int(t1))\n",
    "    time_list.append(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(720, 251, 601)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testarr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# prepare masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flist_raw = !aws s3 ls s3://pepper-dataset/county_mask_data/mask_2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141\n"
     ]
    }
   ],
   "source": [
    "# 所有mask文件\n",
    "pad = '2024-06-16 02:14:19      23897 '\n",
    "len(pad)\n",
    "mask_list = []\n",
    "for f in flist_raw:\n",
    "    if \"tif\" in f:\n",
    "        fname = f[31:]\n",
    "        fpath = f\"/vsis3/pepper-dataset/county_mask_data/mask_2/{fname}\"\n",
    "        mask_list.append(fpath)\n",
    "mask_list.sort()\n",
    "print(len(mask_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 40607,
     "status": "ok",
     "timestamp": 1717284184051,
     "user": {
      "displayName": "Yuhua Situ",
      "userId": "08664026452793101142"
     },
     "user_tz": 300
    },
    "id": "p1dUPA7ayICS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 遍历所有 mask 文件，获取某个county的这个变量的数据\n",
    "mask_dict = {}\n",
    "for mask_file in mask_list:\n",
    "    fname = os.path.basename(mask_file)\n",
    "    location = fname.split('.')[0]\n",
    "    mask_dict[location] = {}\n",
    "    # read mask tiff\n",
    "    gd = gdal.Open(mask_file)\n",
    "    garr = gd.GetRasterBand(1).ReadAsArray()\n",
    "\n",
    "    mask_dict[location] = garr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do masking with gdal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_time(gds_subset):\n",
    "    gds = gdal.Open(gds_subset)\n",
    "    gds_meta = gds.GetMetadata()\n",
    "    time_set = eval(gds_meta['NETCDF_DIM_time_VALUES'])\n",
    "    time_list = []\n",
    "    for t1 in time_set:\n",
    "        t2 = dt.datetime(1900,1,1) + dt.timedelta(hours = int(t1))\n",
    "        time_list.append(t2)\n",
    "    del gds\n",
    "    return time_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# year = 2020\n",
    "# year_path = eval(\"f's3://pepper-dataset/era5-unzipped/{year}/'\")\n",
    "# year_path\n",
    "# flist_raw = !aws s3 ls $year_path\n",
    "# flist = []\n",
    "# for f in flist_raw:\n",
    "#     f2 = f.split(' ')[-1]\n",
    "#     f3 = f'/vsis3/pepper-dataset/era5-unzipped/{year}/{f2}'\n",
    "#     flist.append(f3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 289764,
     "status": "ok",
     "timestamp": 1717298058303,
     "user": {
      "displayName": "Yuhua Situ",
      "userId": "08664026452793101142"
     },
     "user_tz": 300
    },
    "id": "fPILcGAzyUFS",
    "outputId": "1c16eb47-e66e-4cdb-faf3-4feb16fabd5c",
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "`/vsis3/pepper-dataset/Temp/2000/2000-01_part1.nc' not recognized as being in a supported file format. It could have been recognized by driver GRIB, but plugin gdal_GRIB.so is not available in your installation. You may install it with 'conda install -c conda-forge libgdal-grib'. The GDAL_DRIVER_PATH configuration option is not set.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m outd \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m: []}\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# read basic variables\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m gds \u001b[38;5;241m=\u001b[39m \u001b[43mgdal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOpen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mncf\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     36\u001b[0m gds_subsets \u001b[38;5;241m=\u001b[39m gds\u001b[38;5;241m.\u001b[39mGetSubDatasets()\n\u001b[1;32m     37\u001b[0m var_dict \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/osgeo/gdal.py:9192\u001b[0m, in \u001b[0;36mOpen\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   9188\u001b[0m _WarnIfUserHasNotSpecifiedIfUsingExceptions()\n\u001b[1;32m   9189\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gdal\n\u001b[0;32m-> 9192\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_gdal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOpen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: `/vsis3/pepper-dataset/Temp/2000/2000-01_part1.nc' not recognized as being in a supported file format. It could have been recognized by driver GRIB, but plugin gdal_GRIB.so is not available in your installation. You may install it with 'conda install -c conda-forge libgdal-grib'. The GDAL_DRIVER_PATH configuration option is not set."
     ]
    }
   ],
   "source": [
    "geotrans = [-125.05, 0.1, 0, 50.05, 0, -0.1]\n",
    "vlist2 = ['evabs', 'evavt', 'sp', 'sshf', 'ssrd', 'strd', 'tp']\n",
    "vlist1 = ['t2m', 'rsn', 'sde', 'stl1', 'stl2', 'stl3', 'stl4', 'tsn', 'swvl1', 'swvl2', 'swvl3', 'swvl4']\n",
    "\n",
    "with open('./mask_log_Harry_1.log','a') as fp:\n",
    "    fp.writelines('='*10+'\\n')\n",
    "\n",
    "for year in range(2000, 2001):\n",
    "    # 获取某一年的文件\n",
    "    year_path = eval(\"f's3://pepper-dataset/Temp/{year}/'\")\n",
    "    flist_raw = !aws s3 ls $year_path\n",
    "    flist = []\n",
    "    for f in flist_raw:\n",
    "        f2 = f.split(' ')[-1]\n",
    "        if '.nc' not in f2:\n",
    "            continue\n",
    "        f3 = f'/vsis3/pepper-dataset/Temp/{year}/{f2}'\n",
    "        flist.append(f3)\n",
    "    flist.sort()\n",
    "\n",
    "    # 遍历该年的所有netcdf文件， 确定是part1还是part2\n",
    "    for ncf in flist:\n",
    "        ncfname = os.path.basename(ncf)\n",
    "        yearmonth = ncfname.split('_')[0]\n",
    "        month_int = int(yearmonth.split('-')[1])\n",
    "\n",
    "        # 跳过已经完成的部分\n",
    "        # if year == 2020:\n",
    "        #     if month_int not in [9, 10]:\n",
    "        #         continue\n",
    "        # 所有county数据保存在 2 个大表里\n",
    "        # 每个大表都有一个 time 列表，然后每个 county 单独一个 dict\n",
    "        outd = {'time': []}\n",
    "        # read basic variables\n",
    "        gds = gdal.Open(ncf) \n",
    "        gds_subsets = gds.GetSubDatasets()\n",
    "        var_dict = {}\n",
    "        var_all = []\n",
    "        for gds_ss in gds_subsets:\n",
    "            v = gds_ss[0].split(':')[-1]\n",
    "            var_all.append(v)\n",
    "            var_dict[v] = gds_ss[0]\n",
    "\n",
    "        # set variable list \n",
    "        if ncf.split('_')[-1] == 'part1.nc':\n",
    "            vlist = vlist1\n",
    "        elif ncf.split('_')[-1] == 'part2.nc':\n",
    "            vlist = vlist2\n",
    "        else:\n",
    "            raise ValueError('Unknown file type')\n",
    "\n",
    "        # 打开netcdf文件，先获取时间信息\n",
    "        timelist = get_time(var_dict[var_all[0]])\n",
    "        outd['time'].extend(timelist)\n",
    "\n",
    "        # 遍历所有变量名，获取全美的数据\n",
    "        for v in vlist:\n",
    "            with open('./mask_log_Harry_1.log','a') as fp:\n",
    "                now_str = dt.datetime.strftime(dt.datetime.now(), '%Y%m%dT%H%M%S')\n",
    "                fp.writelines(f'{ncf} - {v} - {now_str}\\n')\n",
    "\n",
    "            gds = gdal.Open(var_dict[v])\n",
    "            arr = gds.ReadAsArray()\n",
    "\n",
    "            # arr = ds.variables[v][:]   # [time, lat, lon]\n",
    "            # arr2 = cp.asarray(arr1.data)\n",
    "            # arr = cp.where(arr2 < -30000, cp.nan, arr2)\n",
    "\n",
    "            for location in mask_dict.keys():\n",
    "                print(ncf)\n",
    "                print(v)\n",
    "                print(location)\n",
    "                if location not in outd.keys():\n",
    "                    outd[location] = {}\n",
    "                if v not in outd[location].keys():\n",
    "                    outd[location][v] = np.array([])\n",
    "                garr1 = mask_dict[location]\n",
    "                garr_inds = np.where(garr1 > -10)\n",
    "                # arr_masked = garr1 * arr\n",
    "                arr_masked = arr[:, garr_inds[0], garr_inds[1]]\n",
    "                arr_out = np.mean(arr_masked, axis=1)\n",
    "                # arr_out = cp.nanmean(arr_masked, axis=(1,2))\n",
    "                # print('einsum')\n",
    "                # arr_out = np.einsum(\"ijk,jk->ijk\", arr, garr)\n",
    "                # arr_out = np.einsum(\"ijk->i\", arr_masked)\n",
    "                print('saving to outd (dictionary)')\n",
    "                outd[location][v] = np.concatenate(\n",
    "                    [outd[location][v], arr_out],\n",
    "                    axis=0\n",
    "                    )\n",
    "                clear_output(wait=True)\n",
    "\n",
    "        # 每个月保存一次\n",
    "        partname = ncfname.split('_')[1].split('.')[0]\n",
    "        errlist = []\n",
    "        for location in outd.keys():\n",
    "            if location == 'time':\n",
    "                continue\n",
    "            os.makedirs(f'/home/ec2-user/SageMaker/pepper/county_env3/{location}', exist_ok=True)\n",
    "            try:\n",
    "                outd1 = {}\n",
    "                for key in outd[location].keys():\n",
    "                    outd1[key] = outd[location][key]# .get()\n",
    "                t2 = pd.DataFrame(outd1)\n",
    "                t2['datetime'] = outd['time']\n",
    "                t2.to_csv(\n",
    "                    f'/home/ec2-user/SageMaker/pepper/county_env3/{location}/{location}_{yearmonth}_{partname}.csv', \n",
    "                    index=False\n",
    "                )\n",
    "            except:\n",
    "                print(f'error with location: {location}')\n",
    "                errlist.append(location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'/home/ec2-user/SageMaker/pepper/county_env3/{location}/{location}_{yearmonth}_{partname}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# do masking with netcdf4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VtSGtlMc4YsU"
   },
   "outputs": [],
   "source": [
    "flist_raw = !aws s3 ls s3://pepper-dataset/era5-unzipped/2020/\n",
    "flist = []\n",
    "for f in flist_raw:\n",
    "    f2 = f.split(' ')[-1]\n",
    "    f3 = f'/vsis3/pepper-dataset/era5-unzipped/{f2}'\n",
    "    flist.append(f3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geotrans = [-125.05, 0.1, 0, 50.05, 0, -0.1]\n",
    "vlist2 = ['evabs', 'evavt', 'sp', 'sshf', 'ssrd', 'strd', 'tp']\n",
    "vlist1 = ['t2m', 'rsn', 'sde', 'stl1', 'stl2', 'stl3', 'stl4', 'tsn', 'swvl1', 'swvl2', 'swvl3', 'swvl4']\n",
    "\n",
    "for year in range(2020, 2021):\n",
    "    # 获取某一年的文件\n",
    "    flist = glob(f'/content/drive/MyDrive/2024/ai_pepper/era5/unzip/{year}/*')\n",
    "    flist.sort()\n",
    "\n",
    "    # 遍历该年的所有netcdf文件， 确定是part1还是part2\n",
    "    for ncf in flist:\n",
    "        ncfname = os.path.basename(ncf)\n",
    "        yearmonth = ncfname.split('_')[0]\n",
    "        month_int = int(yearmonth.split('-')[1])\n",
    "\n",
    "        # 跳过已经完成的部分\n",
    "        if year == 2020:\n",
    "            if month_int not in [9, 10]:\n",
    "                continue\n",
    "        # 所有county数据保存在 2 个大表里\n",
    "        # 每个大表都有一个 time 列表，然后每个 county 单独一个 dict\n",
    "        # 每个 county 下面每个变量是一个表\n",
    "        # all_dict1 = {'time': []}\n",
    "        # all_dict2 = {'time': []}\n",
    "        outd = {'time': []}\n",
    "        # read basic variables\n",
    "        with nc.Dataset(ncf,'r') as ds:\n",
    "            var_all = list(ds.variables.keys())\n",
    "            file_type = 0\n",
    "            if ncf.split('_')[-1] == 'part1.nc':\n",
    "                file_type = 1\n",
    "            elif ncf.split('_')[-1] == 'part2.nc':\n",
    "                file_type = 2\n",
    "            else:\n",
    "                raise ValueError('Unknown file type')\n",
    "            if file_type == 1:\n",
    "                for v in vlist1:\n",
    "                    if v not in var_all:\n",
    "                        raise ValueError(f'Part1 file. Variable not complete. Missing: {v}')\n",
    "            elif file_type == 2:\n",
    "                for v in vlist2:\n",
    "                    if v not in var_all:\n",
    "                        raise ValueError(f'Part2 file. Variable not complete. Missing: {v}')\n",
    "            else:\n",
    "                raise ValueError('Error during variable completeness check')\n",
    "\n",
    "        # set variable list\n",
    "        if file_type == 1:\n",
    "            vlist = vlist1\n",
    "            # continue\n",
    "        elif file_type == 2:\n",
    "            vlist = vlist2\n",
    "\n",
    "        # 打开netcdf文件，先获取时间信息\n",
    "        with nc.Dataset(ncf,'r') as ds:\n",
    "            timevar = ds.variables['time'][:]\n",
    "            timelist = [\n",
    "                dt.datetime(1900, 1, 1, 0, 0) + dt.timedelta(hours = int(x)) for x in timevar\n",
    "                ]\n",
    "\n",
    "            # 根据 file_type 选择添加数据到 all_dict1 还是 all_dict2\n",
    "            if file_type == 1:\n",
    "                vlist = vlist1\n",
    "            else:\n",
    "                vlist = vlist2\n",
    "            outd['time'].extend(timelist)\n",
    "            # 遍历所有变量名，获取全美的数据\n",
    "            for v in vlist:\n",
    "                with open(f'/content/drive/MyDrive/2024/ai_pepper/era5/mask2_log_YuhuaSitu2.log','a') as fp:\n",
    "                    now_str = dt.datetime.strftime(dt.datetime.now(), '%Y%m%dT%H%M%S')\n",
    "                    fp.writelines(f'{ncf} - {v} - {now_str}\\n')\n",
    "                arr = ds.variables[v][:]   # [time, lat, lon]\n",
    "                # arr2 = cp.asarray(arr1.data)\n",
    "                # arr = cp.where(arr2 < -30000, cp.nan, arr2)\n",
    "\n",
    "                for location in mask_dict.keys():\n",
    "                    print(ncf)\n",
    "                    print(v)\n",
    "                    print(location)\n",
    "                    if location not in outd.keys():\n",
    "                        outd[location] = {}\n",
    "                    if v not in outd[location].keys():\n",
    "                        outd[location][v] = np.array([])\n",
    "                    garr1 = mask_dict[location]\n",
    "                    garr_inds = np.where(garr1 > -10)\n",
    "                    # arr_masked = garr1 * arr\n",
    "                    arr_masked = arr[:, garr_inds[0], garr_inds[1]]\n",
    "                    arr_out = np.mean(arr_masked, axis=1)\n",
    "                    # arr_out = cp.nanmean(arr_masked, axis=(1,2))\n",
    "                    # print('einsum')\n",
    "                    # arr_out = np.einsum(\"ijk,jk->ijk\", arr, garr)\n",
    "                    # arr_out = np.einsum(\"ijk->i\", arr_masked)\n",
    "                    print('saving to outd (dictionary)')\n",
    "                    outd[location][v] = np.concatenate(\n",
    "                        [outd[location][v], arr_out],\n",
    "                        axis=0\n",
    "                        )\n",
    "                    clear_output(wait=True)\n",
    "\n",
    "\n",
    "        # 每个月保存一次\n",
    "        partname = ncfname.split('_')[1].split('.')[0]\n",
    "        errlist = []\n",
    "        for location in outd.keys():\n",
    "            if location == 'time':\n",
    "                continue\n",
    "            os.makedirs(f'/content/drive/MyDrive/2024/ai_pepper/era5/county_env2/{location}', exist_ok=True)\n",
    "            try:\n",
    "                outd1 = {}\n",
    "                for key in outd[location].keys():\n",
    "                    outd1[key] = outd[location][key]# .get()\n",
    "                t2 = pd.DataFrame(outd1)\n",
    "                t2['datetime'] = outd['time']\n",
    "                t2.to_csv(f'/content/drive/MyDrive/2024/ai_pepper/era5/county_env2/{location}/{location}_{yearmonth}_{partname}.csv', index=False)\n",
    "            except:\n",
    "                print(f'error with location: {location}')\n",
    "                errlist.append(location)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN7LL7U4dGFXM88cN1Xm0xL",
   "mount_file_id": "1o_wzpzbOeZ6BW1ugNEosGSo8xbCVzhVl",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
