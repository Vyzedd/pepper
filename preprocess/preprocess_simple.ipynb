{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从 S3 读取文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import NoCredentialsError\n",
    "import pandas as pd\n",
    "import datetime as dt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取文件列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_region_name = 'us-east-1'\n",
    "bucket_name = 'pepper-dataset'\n",
    "prefix = 'county-env-data/county_env_2_IL_AK_MN/processed_with_all_features/'\n",
    "\n",
    "# Optionally, you can specify the AWS region\n",
    "s3 = boto3.client('s3', region_name=s3_region_name)  # specify region\n",
    "\n",
    "# List to store all object keys\n",
    "county_files = []\n",
    "\n",
    "# Initial call to list_objects_v2\n",
    "response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "\n",
    "# Process the initial response\n",
    "while True:\n",
    "    # Collect keys from current response\n",
    "    current_keys = [obj['Key'] for obj in response.get('Contents', [])]\n",
    "    county_files.extend(current_keys)\n",
    "\n",
    "    # Check if there are more objects to retrieve\n",
    "    if not response['IsTruncated']:\n",
    "        break  # No more objects to retrieve\n",
    "\n",
    "    # Get ContinuationToken for next paginated call\n",
    "    continuation_token = response['NextContinuationToken']\n",
    "\n",
    "    # Make subsequent call with ContinuationToken\n",
    "    response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix, ContinuationToken=continuation_token)\n",
    "    \n",
    "    \n",
    "county_files = [\"/\" + path if not path.startswith(\"/\") else path for path in county_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只选择csv文件\n",
    "county_files_csv = []\n",
    "for f in county_files:\n",
    "    if f.endswith(\".csv\"):\n",
    "        county_files_csv.append(f)\n",
    "print(county_files_csv[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选择年份范围\n",
    "year_range = [2015, 2020]\n",
    "years = [x for x in range(year_range[0], year_range[-1]+1)]\n",
    "\n",
    "filtered_files = [file_path for file_path in county_files_csv\n",
    "                  if any(str(year) in file_path for year in years)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('s3://'+bucket_name+filtered_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = pd.DataFrame(columns=(\"Year\",\"State\",\"County\",\"Month\",\"Commodity\",\"Feature\"))\n",
    "\n",
    "for csvf in filtered_files:\n",
    "    # only read part1, then match with part2\n",
    "    if 'part2' in csvf:\n",
    "        continue\n",
    "    # print(csvf)\n",
    "    crop = \"UNKNOWN\"\n",
    "\n",
    "    suffix = csvf.split('/')[-1]\n",
    "    state = suffix.split('_')[-4].upper()\n",
    "    # print(state)\n",
    "    if state != \"MINNESOTA\":\n",
    "        continue\n",
    "    county_name = suffix.split('_')[-3].upper()\n",
    "    ym_str = csvf.split('_')[-2]\n",
    "\n",
    "    year = int(suffix.split('-')[0][-4:])\n",
    "    month = int(suffix.split('-')[1][:2])\n",
    "    ym_obj = dt.datetime.strptime(ym_str, '%Y-%m')\n",
    "    if ym_obj.year not in years:\n",
    "        continue\n",
    "    # else:\n",
    "    #     print(ym_str)\n",
    "        # pass\n",
    "\n",
    "    df1 = pd.read_csv('s3://'+bucket_name+csvf)\n",
    "    # df1.dropna(axis=1)\n",
    "    csvf_part2 = csvf.replace('part1', 'part2')\n",
    "    df2 = pd.read_csv('s3://'+bucket_name+csvf_part2)\n",
    "    # df2.dropna(axis=1)\n",
    "\n",
    "\n",
    "    df12 = pd.merge(df1, df2, on = 'datetime')\n",
    "    \n",
    "    df12 = df12.drop(columns=['sshf'])\n",
    "    df12 = df12.drop(columns=['evavt'])\n",
    "    \n",
    "    df12['datetime'] = pd.to_datetime(df12['datetime'])\n",
    "\n",
    "    # df12 = df12[df12['datetime'].dt.month >= 5]\n",
    "    # print(df12['datetime'].dt.month)\n",
    "    df12 = df12[(df12['datetime'].dt.month >= 5) & (df12['datetime'].dt.month <= 11)]\n",
    "    if df12.empty:\n",
    "        print(\"No data available after filtering.\")\n",
    "        continue\n",
    "      # drop datetime\n",
    "    df12 = df12.drop(columns=['datetime'])\n",
    "\n",
    "    # print(\"-----------------------\")\n",
    "    # print(df12.shape)\n",
    "    row = [year,state,county_name,month,crop]\n",
    "    row.append(df12)\n",
    "    # print(row[:4])\n",
    "\n",
    "    # df_list.append(df12)\n",
    "    dfLength = len(df_list)\n",
    "    df_list.loc[dfLength] = row\n",
    "\n",
    "print(\"Number of Features Per County Per Year:  >= \",len(df_list.iloc[1,4]))\n",
    "\n",
    "# df_list.to_csv('/content/drive/MyDrive/ai/era5/dataSat_cleaned_Iowa_2015-2020.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.getsizeof(df_list) / 1024 / 1024  # Byte -> MB\n",
    "\n",
    "print(df_list.shape)\n",
    "grouped_data = df_list.groupby(['Year', 'State', 'County']).size().reset_index(name='Count')\n",
    "print(grouped_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取作物产量数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read crop yield (cy) data\n",
    "year_range = [2015, 2020]\n",
    "years = [x for x in range(year_range[0], year_range[-1]+1)]\n",
    "\n",
    "def is_convertible_to_int(s):\n",
    "    try:\n",
    "        int(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "# print((cy_file_list))\n",
    "bucket_name = 'pepper-dataset'\n",
    "prefix = 'crop_data/crop_3/'\n",
    "\n",
    "# List to store all object keys\n",
    "cy_file_list = []\n",
    "\n",
    "# Initial call to list_objects_v2\n",
    "response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "\n",
    "# Process the initial response\n",
    "while True:\n",
    "    # Collect keys from current response\n",
    "    current_keys = [obj['Key'] for obj in response.get('Contents', [])]\n",
    "    cy_file_list.extend(current_keys)\n",
    "\n",
    "    # Check if there are more objects to retrieve\n",
    "    if not response['IsTruncated']:\n",
    "        break  # No more objects to retrieve\n",
    "\n",
    "    # Get ContinuationToken for next paginated call\n",
    "    continuation_token = response['NextContinuationToken']\n",
    "\n",
    "    # Make subsequent call with ContinuationToken\n",
    "    response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix, ContinuationToken=continuation_token)\n",
    "    \n",
    "cy_file_list = [\"/\" + path if not path.startswith(\"/\") else path for path in cy_file_list]\n",
    "\n",
    "print(cy_file_list)\n",
    "cy_df_list = []\n",
    "for cy_file in cy_file_list:\n",
    "    suffix = cy_file.split('/')[-1]\n",
    "    temp = suffix.split('_')\n",
    "    state = temp[0].upper()\n",
    "    \n",
    "    if state == \"MINNESOTA\":\n",
    "        crop_type = temp[1]\n",
    "        df = pd.read_csv('s3://'+bucket_name+cy_file)\n",
    "        df = df[(df['Year'] >= 2015) & (df['Year'] <= 2020)]\n",
    "        # Drop Rows that do not have \"YIELD\" in the \"DATA ITEM\" column\n",
    "        dataYield = df.drop(df[~df[\"Data Item\"].str.contains(\"YIELD\")].index, inplace = False)\n",
    "        # Drop Rows that have \"IRRIGATED\" in the \"DATA ITEM\" column\n",
    "        dataYield = dataYield.drop(dataYield[dataYield[\"Data Item\"].str.contains(\"IRRIGATED\")].index, inplace = False)\n",
    "        cy_df_list.append(dataYield)\n",
    "        \n",
    "\n",
    "cy_df_all = cy_df_list[0]\n",
    "for df in cy_df_list[1:]:\n",
    "    cy_df_all = pd.concat([cy_df_all, df], axis = 0)\n",
    "print(cy_df_all.shape)\n",
    "\n",
    "print(cy_df_all.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataROI = cy_df_all[[\"Year\",\"State\",\"County\",\"Commodity\",\"Value\"]]\n",
    "name_year = dataROI[\"Year\"].drop_duplicates()\n",
    "name_states = dataROI[\"State\"].drop_duplicates()\n",
    "name_county = dataROI[\"County\"].drop_duplicates()\n",
    "name_crops = dataROI[\"Commodity\"].drop_duplicates()\n",
    "\n",
    "dfOut = pd.DataFrame(columns=(\"Year\",\"State\",\"County\",\"Commodity\",\"Yield\"))\n",
    "\n",
    "# For each year\n",
    "idx = 0\n",
    "for year in name_year:\n",
    "  temp_year = dataROI.loc[dataROI[\"Year\"]==year]\n",
    "  # if debug and verbose: print(temp_year)\n",
    "  # For each state\n",
    "  for state in name_states:\n",
    "    temp_state = temp_year.loc[temp_year[\"State\"]==state]\n",
    "    # if debug and verbose: print(temp_state)\n",
    "    # For each county\n",
    "    for county in name_county:\n",
    "      temp_county = temp_state.loc[temp_state[\"County\"]==county]\n",
    "      # if debug and verbose: print(temp_county)\n",
    "      # For each crop\n",
    "      for crop in name_crops:\n",
    "        temp_crops = temp_county.loc[temp_county[\"Commodity\"]==crop]\n",
    "        # if debug and verbose: print(temp_crops)\n",
    "        crop_yield = temp_crops[\"Value\"]\n",
    "        crop_yield = crop_yield.replace(',','', regex=True)\n",
    "        crop_yield = pd.to_numeric(crop_yield)\n",
    "        crop_yield = crop_yield.sum()\n",
    "        # if debug and verbose: print(crop_yield)\n",
    "\n",
    "        # Add to Output Data Frame\n",
    "        dfOut.loc[idx] = [year,state,county,crop,crop_yield]\n",
    "\n",
    "        # Increment idx\n",
    "        idx = idx + 1\n",
    "\n",
    "print(dfOut.head())\n",
    "# wheat_rows = dfOut[dfOut['Commodity'] == 'WHEAT']\n",
    "# print(\"wheat_rows: \", len(wheat_rows))\n",
    "dfOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runCorrelateFeatureLabel(dataLabel,dataFeature,crop=\"WHEAT\",dropZeroYield=True,debug=False,verbose=False):\n",
    "  \"\"\"Takes 2 pandas DataFrames (Label & Feature) and pairs them together\"\"\"\n",
    "\n",
    "  # Drop Rows without the Desired Crop\n",
    "  # print(\"pre dataCrop:\", dataLabel.head())\n",
    "  dataCrop = dataLabel.drop(dataLabel[~dataLabel[\"Commodity\"].str.contains(\"CORN\")].index, inplace = False)\n",
    "  print(\"pre2 dataCrop:\", dataCrop.head())\n",
    "  dataCrop = dataCrop.drop(columns=[\"Commodity\"]) #TODO: Assume we are only using 1 commodity\n",
    "  dataFeat = dataFeature.drop(columns=[\"Commodity\"])\n",
    "  print(\"dataCrop:\", dataCrop.head())\n",
    "  print(\"dataFeat:\", dataFeat.head())\n",
    "  # print(\"++++++++++++++++++\")\n",
    "\n",
    "  # Merge DataFrames\n",
    "  # Ref: https://realpython.com/pandas-merge-join-and-concat/\n",
    "  # dataCrop.index = dataCrop.index.astype('object')\n",
    "  print(\"------1: \", dataCrop.dtypes)\n",
    "  print(\"------2: \", dataFeat.dtypes)\n",
    "  dataFeat['Year'] = dataFeat['Year'].astype('int64')\n",
    "  dataMerge = dataCrop.merge(dataFeat,how='right',on=['Year','State','County'])\n",
    "  print(dataMerge.head())\n",
    "\n",
    "  # Drop Rows without yield information\n",
    "  if dropZeroYield:\n",
    "    dataOut = dataMerge.drop(dataMerge[dataMerge[\"Yield\"]==0.0].index,inplace=False)\n",
    "  else:\n",
    "    dataOut = dataMerge\n",
    "\n",
    "  # Drop Rows with NaN\n",
    "  dataOut.dropna(subset=['Yield'],inplace=True)\n",
    "\n",
    "  if debug: print(dataOut)\n",
    "\n",
    "  return dataOut\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# print(\"row shape:\", df_list.iloc[1][4].info())\n",
    "print(dfOut.shape)\n",
    "print(dfOut.head())\n",
    "print(df_list.shape)\n",
    "print(df_list.head())\n",
    "print(df_list.iloc[1][5].shape)\n",
    "print(\"---------------------+\")\n",
    "# print(df_list.iloc[1])\n",
    "dataSet = runCorrelateFeatureLabel(dfOut,df_list,crop=\"CORN\",dropZeroYield=True,debug=False)\n",
    "\n",
    "print(\"------------------\")\n",
    "print(dataSet.shape)\n",
    "print(dataSet.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveCorrelatedData(data,savePath,saveName,debug=False):\n",
    "  \"\"\"Save the correlated data to a mat for easier processing\"\"\"\n",
    "\n",
    "  # Extract from DataFrame as list\n",
    "  feature = data['Feature']\n",
    "  print(\"feature len: \", len(feature))\n",
    "  label = data['Yield']\n",
    "  print(\"label len: \", len(label))\n",
    "  # print(label)\n",
    "  month = data['Month']\n",
    "  if debug: print(len(feature))\n",
    "\n",
    "  feat_lst = []\n",
    "  label_lst = []\n",
    "  feat_new = []\n",
    "  firstTimeLoop = True\n",
    "  for idx in range(len(feature)):\n",
    "    # if debug: print(idx)\n",
    "    print(\"idx:\", idx)\n",
    "    if int(month.iloc[idx]) == 5:\n",
    "      feat_new = []\n",
    "    for f in feature.iloc[idx]:\n",
    "      # print(f)\n",
    "      # feat_new.append(feature.iloc[idx][f])\n",
    "      for item in feature.iloc[idx][f]:\n",
    "        feat_new.append(item)\n",
    "    # feat_new = np.array(feat_new)\n",
    "\n",
    "    # Drop Bad Datasets\n",
    "    # if len(feat_new) != 20: #4320: for whole year, 2520 for Jan-Jul, 3240 for Nov-Jul\n",
    "    #   if debug: print(\"skipping feature of length\",len(feat_new))\n",
    "    #   continue\n",
    "\n",
    "    if int(month.iloc[idx]) != 11:\n",
    "      continue\n",
    "\n",
    "    if len(feat_new) != 87312:  # 检查 feat_new 列表的长度是否为 240\n",
    "        print(\"Skipping feature at index\", idx, \"due to incorrect length:\", len(feat_new))\n",
    "        continue\n",
    "\n",
    "    print(\"feat_new len:\", len(feat_new))\n",
    "    # print(\"feat_arr:\", len(feat_arr))\n",
    "    # Append Features\n",
    "    feat_lst.append(feat_new)\n",
    "    label_lst.append(label.iloc[idx])\n",
    "\n",
    "  print(len(label_lst))\n",
    "  # feat_arr = np.array(feat_lst)\n",
    "  feat_arr = feat_lst\n",
    "  # print(feat_arr[0])\n",
    "\n",
    "  data_feature_expanded = pd.DataFrame(feat_arr)\n",
    "\n",
    "  # 将 DataFrame 写入 CSV 文件\n",
    "  print(\"start writing dataFrame to csv: \", 's3://'+bucket_name+'/MINNESOTA_data_feature_expanded_CORN.csv')\n",
    "  data_feature_expanded.to_csv('s3://'+bucket_name+'/MINNESOTA_data_feature_expanded_CORN.csv', index=False)\n",
    "\n",
    "  print(\"Data successfully written to 'MINNESOTA data_feature_expanded.csv'\")\n",
    "\n",
    "  # Get Ready to Save\n",
    "  # feature = feat_arr.tolist()\n",
    "  label = label_lst\n",
    "  data_label_expanded = pd.DataFrame(label)\n",
    "  data_label_expanded.to_csv('s3://'+bucket_name+'/MINNESOTA_data_label_expanded_CORN.csv', index=False)\n",
    "  print(\"Data successfully written to 'MINNESOTA_data_label_expanded.csv'\")\n",
    "\n",
    "  # 将 DataFrame 写入 CSV 文件\n",
    "  # df.to_csv(filename, index=False, header=False)\n",
    "  print(\"++++++++++++\")\n",
    "  # print(\"feat_arr shape: \", feat_arr.shape)\n",
    "  # print(feat_arr.head())\n",
    "  print(len(feat_arr))\n",
    "  print(len(feat_arr[0]))\n",
    "  print(label)\n",
    "\n",
    "\n",
    "  return label, feat_arr\n",
    "\n",
    "\n",
    "# print(dataSet.iloc[1][4].shape())\n",
    "\n",
    "label, feat_arr = saveCorrelatedData(dataSet,\"/content/drive/MyDrive/ai/era5/\",\"yield_Iowa_corn_noIrr.mat\",debug=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
