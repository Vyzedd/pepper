{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 复制出test-state_corn里面的训练部分的代码，不运行，主要整理格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # Handle OS file systems and directory paths\n",
    "import random # Random number\n",
    "import numpy as np # Import NumPy for arrays and such\n",
    "import matplotlib.pyplot as plt # Plotting Tools\n",
    "import numpy.linalg as npl # Linear Algebra Library\n",
    "import time # Time\n",
    "\n",
    "import pandas as pd # Import Pandas for data processing\n",
    "import datetime as dt\n",
    "import scipy.io as sio\n",
    "# import io\n",
    "from decimal import Decimal # String to double conversion\n",
    "from glob import glob  # 从 glob 模块中导入 glob 函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Input, Dense, LSTM, MaxPooling1D, Conv1D\n",
    "from keras.models import Model\n",
    "import keras\n",
    "from keras.layers import Dense, TimeDistributed\n",
    "from keras.models import Sequential\n",
    "# from keras.utils import to_categorical\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "# from keras.utils import np_utils\n",
    "import itertools\n",
    "\n",
    "from keras.layers import LSTM, RepeatVector\n",
    "# from keras.layers.convolutional import Conv1D,Conv2D\n",
    "# from keras.layers.convolutional import MaxPooling1D,MaxPooling2D\n",
    "from keras.layers import Dropout, Flatten\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler # for normalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from numpy import dstack,  hstack\n",
    "import scipy.io as sio\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# data_mat = \"/content/drive/MyDrive/ai/era5/yield_Iowa_corn_noIrr.mat\" #Name of File, may need to specify path\n",
    "# data_dict = sio.loadmat(data_mat) #Load MAT as a python dictionary\n",
    "# print(data_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_feature = pd.read_csv('s3://'+bucket_name+ '/NORTH_DAKOTA_data_feature_expanded_CORN.csv')\n",
    "data_label = pd.read_csv('s3://'+bucket_name+ '/NORTH_DAKOTA_data_label_expanded_CORN.csv')\n",
    "\n",
    "# data_feature = pd.read_csv('D:/data/crop_ai/NORTH_DAKOTA_data_feature_expanded_CORN.csv')\n",
    "# data_label = pd.read_csv('D:/data/crop_ai/NORTH_DAKOTA_data_label_expanded_CORN.csv')\n",
    "\n",
    "print('Data Feature Shape:',data_feature.shape) # Verify Shape\n",
    "print('Data Label Shape:',data_label.shape) # Verify Shape\n",
    "\n",
    "\n",
    "train_X = data_feature\n",
    "train_y = data_label.T\n",
    "\n",
    "train_X = np.nan_to_num(train_X)\n",
    "train_y = np.nan_to_num(train_y)\n",
    "\n",
    "print(train_X.shape)\n",
    "print(train_y.shape)\n",
    "\n",
    "x = np.asarray(train_X, dtype=np.float32)\n",
    "y = np.asarray(train_y).flatten()\n",
    "\n",
    "# Training/Validation split 67%, 33% split\n",
    "data_feature, X_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)\n",
    "print(X_test.shape)\n",
    "print(data_label.shape)\n",
    "print(data_feature.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reshape 训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataReshape(dataIn,debug=True,runOnce=False):\n",
    "  \"\"\"Takes the 1D feature array and reshapes to 270x16\"\"\"\n",
    "  print(dataIn.shape)\n",
    "\n",
    "  dataOut = []\n",
    "  for idx in range(len(dataIn)):\n",
    "    dataTemp = dataIn[idx].reshape(5457, 16) #(216,15) 3240\n",
    "    if debug: print(dataTemp.shape)\n",
    "    dataOut.append(dataTemp)\n",
    "\n",
    "    if runOnce: return -1\n",
    "\n",
    "  return np.array(dataOut)\n",
    "\n",
    "data_feature_rs = dataReshape(data_feature,debug=False,runOnce=False)\n",
    "print(data_feature_rs.shape)\n",
    "\n",
    "#Reshape test\n",
    "X_test = dataReshape(X_test,debug=False,runOnce=False)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 转化为TF数据集，设置训练超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Parameters\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 200\n",
    "SHUFFLE_BUFFER_SIZE = 64\n",
    "\n",
    "# Normalize Data\n",
    "normalizer = Normalization(axis=-1)\n",
    "normalizer.adapt(data_feature_rs)\n",
    "data_feature_norm = normalizer(data_feature_rs)\n",
    "\n",
    "# Normalize Test\n",
    "normalizer = Normalization(axis=-1)\n",
    "normalizer.adapt(X_test)\n",
    "X_test_norm = normalizer(X_test)\n",
    "\n",
    "\n",
    "# Split into train and test datasets\n",
    "# Load the Data into a data loader\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((data_feature_norm,y_train.T))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test_norm,y_test.T)) #TODO: REPLACE WITH TEST DATASET\n",
    "DATASET_SIZE = len(data_feature_norm)\n",
    "\n",
    "# Shuffle & Batch the Datasets\n",
    "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def step_decay(epoch):\n",
    "   initial_lrate = 0.1\n",
    "   drop = 0.5\n",
    "   epochs_drop = 10.0\n",
    "   lrate = initial_lrate * np.pow(drop,\n",
    "           np.floor((1+epoch)/epochs_drop))\n",
    "   return lrate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建网络，普通"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Network\n",
    "visible = Input(shape= (5457, 16))# (270, 16))\n",
    "\n",
    "filters1 = 80\n",
    "filters2 = 64\n",
    "\n",
    "\n",
    "cnn1 = Conv1D(filters=80, kernel_size=2, activation='relu')(visible)\n",
    "cnn1 = Dense(64)(cnn1)\n",
    "\n",
    "cnn1 = MaxPooling1D(pool_size=128)(cnn1)\n",
    "\n",
    "cnn2 = Conv1D(filters=64, kernel_size=2, activation='relu')(visible)\n",
    "cnn2 = Dense(64)(cnn2)\n",
    "\n",
    "cnn2 = MaxPooling1D(pool_size=128)(cnn2)\n",
    "\n",
    "cnn3 = Conv1D(filters=32, kernel_size=2, activation='relu')(visible)\n",
    "cnn3 = Dense(64)(cnn3)\n",
    "cnn3 = MaxPooling1D(pool_size=128)(cnn3)\n",
    "\n",
    "cnn4 =Conv1D(filters=16, kernel_size=2,activation='relu')(visible)\n",
    "cnn4 = Dense(64)(cnn4)\n",
    "cnn4 = MaxPooling1D(pool_size=128)(cnn4)\n",
    "merged = keras.layers.concatenate([cnn1, cnn2,  cnn3, cnn4], axis=2)\n",
    "cnn = Dense(16)(merged)\n",
    "# cnn = LSTM(128,recurrent_dropout=0.1)(cnn)\n",
    "# cnn =Dense(4)(cnn)\n",
    "cnn = Flatten()(cnn)\n",
    "output1 =Dense(1,activation='linear')(cnn)\n",
    "\n",
    "\n",
    "# tie together\n",
    "model = Model(inputs=visible, outputs=output1)\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics =['accuracy','mean_squared_error','mean_absolute_percentage_error',tf.keras.metrics.RootMeanSquaredError()])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建网络，resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv1D, BatchNormalization, Activation, Add\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, MaxPooling1D, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "def residual_block(input_tensor, filters, kernel_size=2):\n",
    "    x = Conv1D(filters=filters, kernel_size=kernel_size, padding='same')(input_tensor)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = Conv1D(filters=filters, kernel_size=kernel_size, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    # Add the input tensor back to the output\n",
    "    x = Add()([x, input_tensor])\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "# Input layer\n",
    "visible = Input(shape=(5457, 16))\n",
    "\n",
    "# First ResNet Pathway\n",
    "cnn1 = Conv1D(filters=80, kernel_size=2, padding='same', activation='relu')(visible)\n",
    "cnn1 = residual_block(cnn1, 80)\n",
    "cnn1 = MaxPooling1D(pool_size=128)(cnn1)\n",
    "\n",
    "# Second ResNet Pathway\n",
    "cnn2 = Conv1D(filters=64, kernel_size=2, padding='same', activation='relu')(visible)\n",
    "cnn2 = residual_block(cnn2, 64)\n",
    "cnn2 = MaxPooling1D(pool_size=128)(cnn2)\n",
    "\n",
    "# Third ResNet Pathway\n",
    "cnn3 = Conv1D(filters=32, kernel_size=2, padding='same', activation='relu')(visible)\n",
    "cnn3 = residual_block(cnn3, 32)\n",
    "cnn3 = MaxPooling1D(pool_size=128)(cnn3)\n",
    "\n",
    "# Fourth ResNet Pathway\n",
    "cnn4 = Conv1D(filters=16, kernel_size=2, padding='same', activation='relu')(visible)\n",
    "cnn4 = residual_block(cnn4, 16)\n",
    "cnn4 = MaxPooling1D(pool_size=128)(cnn4)\n",
    "\n",
    "# Concatenate the outputs from all paths\n",
    "merged = concatenate([cnn1, cnn2, cnn3, cnn4], axis=2)\n",
    "\n",
    "# Fully connected layer after concatenation\n",
    "cnn = Dense(16)(merged)\n",
    "\n",
    "# Flatten the output\n",
    "cnn = Flatten()(cnn)\n",
    "\n",
    "# Output layer\n",
    "output1 = Dense(1, activation='linear')(cnn)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=visible, outputs=output1)\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics =['accuracy','mean_squared_error','mean_absolute_percentage_error',tf.keras.metrics.RootMeanSquaredError()])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIT model with TRAINING DATA\n",
    "history = model.fit(train_dataset, validation_data=test_dataset,epochs=NUM_EPOCHS,verbose=2)\n",
    "model.summary()\n",
    "\n",
    "# EVALUATE model with TESTING DATA\n",
    "model.evaluate(test_dataset, verbose=0)\n",
    "\n",
    "ynew = model.predict(test_dataset)\n",
    "print(f'Test Loss: {ynew[0]}')\n",
    "print(f'Test Mean Squared Error: {ynew[1]}')\n",
    "print(f'Test Mean Absolute Percentage Error: {ynew[2]}')\n",
    "print(f'Test Root Mean Squared Error: {ynew[3]}')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_title('Training and Validation \\n Mean Absolute Percentage Error') # loss = 100 * abs(y_true - y_pred) / y_true\n",
    "ax.plot(history.history['mean_absolute_percentage_error'], color = 'blue', label = 'train')\n",
    "\n",
    "# plt.title('Validation Mean Absolute Percentage Error (MAPE)') # loss = 100 * abs(y_true - y_pred) / y_true\n",
    "ax.plot(history.history['val_mean_absolute_percentage_error'], color = 'orange', label = 'val')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练，Reduce LR on Plateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIT model with TRAINING DATA\n",
    "# history = model.fit(train_dataset, validation_data=test_dataset,epochs=NUM_EPOCHS,verbose=2)\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# Reduce learning rate when a metric has stopped improving.\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',      # 监控的指标，可以改成其他指标如 'val_mean_squared_error'\n",
    "    factor=0.5,              # 学习率减少的因子，new_lr = lr * factor\n",
    "    patience=10,             # 当监控指标在'patience'个epoch内没有改善时，减少学习率\n",
    "    min_lr=1e-6,             # 学习率的下限\n",
    "    verbose=1                # 设置为1可以看到学习率变化的日志\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['accuracy', 'mean_squared_error', 'mean_absolute_percentage_error', tf.keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "# Train the model with ReduceLROnPlateau callback\n",
    "history = model.fit(\n",
    "    train_dataset, \n",
    "    epochs=NUM_EPOCHS, \n",
    "    validation_data=test_dataset,  # 假设有验证集\n",
    "    callbacks=[reduce_lr]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# EVALUATE model with TESTING DATA\n",
    "model.evaluate(test_dataset, verbose=0)\n",
    "\n",
    "ynew = model.predict(test_dataset)\n",
    "print(f'Test Loss: {ynew[0]}')\n",
    "print(f'Test Mean Squared Error: {ynew[1]}')\n",
    "print(f'Test Mean Absolute Percentage Error: {ynew[2]}')\n",
    "print(f'Test Root Mean Squared Error: {ynew[3]}')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_title('Training and Validation \\n Mean Absolute Percentage Error') # loss = 100 * abs(y_true - y_pred) / y_true\n",
    "ax.plot(history.history['mean_absolute_percentage_error'], color = 'blue', label = 'train')\n",
    "\n",
    "# plt.title('Validation Mean Absolute Percentage Error (MAPE)') # loss = 100 * abs(y_true - y_pred) / y_true\n",
    "ax.plot(history.history['val_mean_absolute_percentage_error'], color = 'orange', label = 'val')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "\n",
    "# 重置数据集\n",
    "def create_test_dataset(X_test_norm, y_test):\n",
    "    return tf.data.Dataset.from_tensor_slices((X_test_norm, y_test.T)).batch(64)\n",
    "\n",
    "# 创建测试数据集\n",
    "test_dataset = create_test_dataset(X_test_norm, y_test)\n",
    "\n",
    "# EVALUATE model with TESTING DATA\n",
    "results = model.evaluate(test_dataset, verbose=0)\n",
    "print(f'Test Loss: {results[0]}')\n",
    "print(f'Test Mean Squared Error: {results[1]}')\n",
    "print(f'Test Mean Absolute Percentage Error: {results[2]}')\n",
    "print(f'Test Root Mean Squared Error: {results[3]}')\n",
    "\n",
    "# PREDICT with TESTING DATA\n",
    "ynew = model.predict(test_dataset)\n",
    "\n",
    "# 重置测试数据集\n",
    "test_dataset = create_test_dataset(X_test_norm, y_test)\n",
    "\n",
    "# EVALUATE model again with TESTING DATA\n",
    "results = model.evaluate(test_dataset, batch_size=64)\n",
    "print(f'loss: {results[0]} - accuracy: {results[1]} - mean_squared_error: {results[2]} - mean_absolute_percentage_error: {results[3]} - root_mean_squared_error: {results[4]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# plt.title('Loss (Mean Squared Error)')\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.figure()\n",
    "# plt.title('Accuracy')\n",
    "# plt.plot(history.history['accuracy'])\n",
    "# plt.figure()\n",
    "# plt.title('Mean Squared Error')\n",
    "# plt.plot(history.history['mean_squared_error'])\n",
    "# plt.figure()\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_title('Training and Validation \\n Mean Squared Error') # loss = 100 * abs(y_true - y_pred) / y_true\n",
    "ax.plot(history.history['loss'], color = 'blue', label = 'train')\n",
    "# plt.figure()\n",
    "## Validation Plots\n",
    "# plt.title('Validation Loss (MSE)') # loss = 100 * abs(y_true - y_pred) / y_true\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.figure()\n",
    "\n",
    "# plt.title('Validation Mean Absolute Percentage Error (MAPE)') # loss = 100 * abs(y_true - y_pred) / y_true\n",
    "ax.plot(history.history['val_loss'], color = 'orange', label = 'val')\n",
    "ax.semilogy((0,50))\n",
    "ax.legend()\n",
    "# plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 计算评估指标\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)  # RMSE\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'MSE: {mse}')\n",
    "print(f'RMSE: {rmse}')\n",
    "print(f'MAE: {mae}')\n",
    "print(f'R^2: {r2}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "frcnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
